import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import json
import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime
import google.generativeai as genai
from supabase import create_client, Client
from dotenv import load_dotenv
from newspaper import Article, Config
from config import GEMINI_MODEL_NAME
from push_notification import send_push_to_all

load_dotenv()
GOOGLE_API_KEY =  os.getenv("GEMINI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
genai.configure(api_key=GOOGLE_API_KEY)
MODEL_NAME = GEMINI_MODEL_NAME
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# --- [ëª¨ë“ˆ 1] ë°ì´í„° ìˆ˜ì§‘ (Collector) ---

def fetch_naver_finance_main():
    print("Fetching Naver Finance Main News...")
    url = "https://finance.naver.com/news/mainnews.naver"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_items = []
        articles = soup.select(".mainNewsList li")
        
        for article in articles[:100]:  # ìƒìœ„ 20ê°œë¡œ ì¶•ì†Œ
            # ì œëª© ì¶”ì¶œ
            title_tag = article.select_one("dd.articleSubject a")
            if not title_tag: # ì¸ë„¤ì¼ êµ¬ì¡°ì¼ ê²½ìš° dt íƒœê·¸ì¼ ìˆ˜ ìˆìŒ
                title_tag = article.select_one("dt.articleSubject a")
            
            # ìš”ì•½(Snippet) ì¶”ì¶œ
            summary_tag = article.select_one("dd.articleSummary")
            
            if title_tag and summary_tag:
                title = title_tag.text.strip()
                link = "https://finance.naver.com" + title_tag['href']
                snippet = summary_tag.text.strip().replace("\n", " ")[:150] # ì• 150ìë§Œ
                
                news_items.append({
                    "title": title,
                    "snippet": snippet,
                    "url": link
                })
                
        return news_items
    except Exception as e:
        print(f"Error fetching Naver: {e}")
        return []

def fetch_yahoo_finance_stable():
    print("Fetching Yahoo Finance Top Stories with newspaper3k...")
    rss_url = "https://finance.yahoo.com/news/rss/topstories"
    
    # ë´‡ íƒì§€ ìš°íšŒë¥¼ ìœ„í•œ ì„¤ì •
    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'
    config = Config()
    config.browser_user_agent = user_agent
    config.request_timeout = 10

    try:
        feed = feedparser.parse(rss_url)
        news_items = []
        
        # ìƒìœ„ 10ê°œë§Œ ìˆ˜ì§‘ (í† í° ì ˆì•½)
        for entry in feed.entries[:30]: 
            try:
                # 1. URL í™•ë³´
                url = entry.link
                
                # 2. Article ê°ì²´ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ (newspaper3kê°€ ì•Œì•„ì„œ ì²˜ë¦¬)
                article = Article(url, config=config)
                article.download() # HTML ë‹¤ìš´ë¡œë“œ
                article.parse()    # ë³¸ë¬¸ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ ê°€ë™
                
                # 3. ë°ì´í„° ì •ì œ (nlp()ë¥¼ í˜¸ì¶œí•˜ë©´ í‚¤ì›Œë“œ/ìš”ì•½ë„ ìë™ ì¶”ì¶œ ê°€ëŠ¥í•˜ì§€ë§Œ ì—¬ê¸°ì„  ìƒëµ)
                full_text = article.text
                
                # ë³¸ë¬¸ì´ ë¹„ì–´ìˆìœ¼ë©´ RSSì˜ summaryë¡œ ëŒ€ì²´
                if not full_text:
                     full_text = entry.get('summary', entry.get('description', ''))

                news_items.append({
                    "title": article.title if article.title else entry.title,
                    "summary": full_text[:300] + "..." if len(full_text) > 300 else full_text, # ì „ì²´ ë³¸ë¬¸ ëŒ€ì‹  ìš”ì•½ë§Œ ì „ì†¡í•˜ë„ë¡ ìˆ˜ì •
                    "url": url
                })
                print(f"Success: {entry.title[:15]}...")
                
            except Exception as e:
                print(f"Failed to parse {entry.link}: {e}")
                # ì‹¤íŒ¨ ì‹œ RSS ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥
                news_items.append({
                    "title": entry.title,
                    "content": entry.get('summary', ''),
                    "url": entry.link
                })

        return news_items

    except Exception as e:
        print(f"RSS Load Error: {e}")
        return []

def process_news_with_gemini(raw_news_list):
    """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ Geminiì—ê²Œ ë³´ë‚´ ì¤‘ìš” ë‰´ìŠ¤ 5ê°œ ì„ ë³„"""
    print("Processing with Gemini AI...")
    
    if not raw_news_list:
        print("No news to process.")
        return []

    model = genai.GenerativeModel(MODEL_NAME)
    
    prompt = f"""
    ë„ˆëŠ” ì „ë¬¸ ê²½ì œ ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼. ì•„ë˜ ì œê³µëœ [ë‰´ìŠ¤ ë°ì´í„°]ëŠ” í•œêµ­ê³¼ ì„¸ê³„ì˜ ì£¼ìš” ê²½ì œ ë‰´ìŠ¤ë“¤ì´ì•¼.
    í•˜ë£¨ì— í•œë²ˆ 5ê°€ì§€ì˜ ì†Œì‹ë§Œ ê³¨ë¼ì„œ ë³´ì—¬ì¤˜ì•¼ í•˜ë‹ˆ, ì´ ì¤‘ì—ì„œ ê°€ì¥ 'ê²½ì œì  íŒŒê¸‰ë ¥(êµµì§í•œ ì†Œì‹)'ì´ í¬ê³  ì¤‘ìš”í•œ ì‚¬ê±´ 5ê°€ì§€ë¥¼ ì„ ë³„í•´ì¤˜. 

    [ìš”êµ¬ì‚¬í•­]
    1. ë‹¤ì–‘ì„±: í•œêµ­(KR)/ë¯¸êµ­(US)/ê¸€ë¡œë²Œ(Global) ì´ìŠˆë¥¼ ì ì ˆíˆ ì„ì–´ì„œ ì´ 5ê°œë¥¼ ë§ì¶°ì¤˜. 
    2. ì¬ê°€ê³µ:
       - `keyword`: ìê·¹ì ì´ì§€ ì•Šê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ëª…í™•í•œ í—¤ë“œë¼ì¸ìœ¼ë¡œ ìƒˆë¡œ ì‘ì„±í•´.
       - `summary`: `- ìš”ì•½ ë‚´ìš© \\n- ì‹œì¥ ì „ë§/ì‹œì‚¬ì (~í•  ì „ë§ì„ì´ ì•„ë‹Œ ê·¸ì € ê°€ëŠ¥ì„±ì´ ìˆì„ìˆ˜ë„ìˆë‹¤ëŠ” ì‹ì˜ ì„œìˆ )` \\n- ì´ ì†Œì‹ì´ ì¤‘ìš”í•œ ì´ìœ  ex>ë‹¹ë¶„ê°„ ëŒ€ì¶œ ì´ìê°€ ë–¨ì–´ì§€ê¸´ í˜ë“¤ë‹¤ëŠ” ëœ»ìœ¼ë¡œ, ì˜ëŒì¡±ì—ê²ŒëŠ” ë¶€ì •ì ì¼ ìˆ˜ ìˆìŒ
       - `links`: ì„ ë³„ëœ ë‰´ìŠ¤ì˜ ì›ë³¸ URLê³¼ ì œëª©ì„ ë°˜ë“œì‹œ ì•„ë˜ ì˜ˆì‹œì™€ ê°™ì€ ê°ì²´ ë°°ì—´ í˜•ì‹ìœ¼ë¡œ í¬í•¨í•´.
    3. ì¶œë ¥ í˜•ì‹: ë°˜ë“œì‹œ ì•„ë˜ JSON í¬ë§·(Array of Objects)ìœ¼ë¡œë§Œ ì¶œë ¥í•´. Markdown ì½”ë“œ ë¸”ëŸ­(```json)ì„ ì“°ì§€ ë§ˆ.
    4. ê° ë‰´ìŠ¤ì˜ `keyword`ì— ë§ˆì§€ë§‰ì— keywordì— ë§ëŠ” ì´ëª¨ì§€ ì‚¬ìš©(ê°ì • ì´ëª¨ì§€ëŠ” ê¸ˆì§€)
    5. summary ìš”ì•½ ì‘ì„±ì‹œ ìµœëŒ€í•œ ë‹¨ì–´ë¡œ ë¬¸ì¥ì„ ëë§ºìŒ, í•œ ì¤„ë§ˆë‹¤ 50ì ì •ë„ë¡œ ì‘ì„±í•  ê²ƒ.
    6. ê¸°ì—…ì— ëŒ€í•œ ë‰´ìŠ¤ê°€ ë‚˜ì˜¬ê²½ìš° categoryëŠ” í•´ë‹¹ ê¸°ì—…ì˜ ì†Œì† êµ­ê°€ë¡œ ë§ì¶œê²ƒ.
    7. ì›ìì œ, ì•”í˜¸í™”í ë‰´ìŠ¤ì˜ categoryëŠ” Globalì„.
    8. íŠ¹ìˆ˜ë¬¸ì **ê°™ì€ ë¬¼ê²°í‘œëŠ” ì‚¬ìš© ê¸ˆì§€**ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ë§Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.



    [ì„ ì • ê¸°ì¤€]
    0. ì§€ìˆ˜ ë³€ë™, í™˜ìœ¨ ë³€ë™, ê¸ˆë¦¬ ë³€ë™ì„ ì•Œë¦¬ëŠ” ì§ì ‘ì ì¸ ë‰´ìŠ¤ì¸ê°€?
    1. ë‰´ìŠ¤ê°€ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë ¥ì´ í°ê°€?
    2. ì „ë§, ì˜ˆì¸¡ë³´ë‹¨ í˜„ì¬ ìƒí™©ì„ ëª…í™•íˆ ì„¤ëª…í•˜ëŠ” ë‰´ìŠ¤ì¸ê°€?
    3. ì§€ìˆ˜, í™˜ìœ¨, ê¸ˆë¦¬, ì¤‘ìš”í•œ ì •ì±… ìœ„ì£¼ì˜ ë‰´ìŠ¤ì¸ê°€?
    4. êµ­ê°€ ì •ì±…, ê¸ˆë¦¬, í™˜ìœ¨ë“± ê³¼ ê°™ì€ ì£¼ìš” ì´ìŠˆì¸ê°€?
    5. ê¸€ë¡œë²Œ ë¹…í…Œí¬ë‚˜ ì£¼ìš” ì‚°ì—…ì˜ íŒë„ë¥¼ ë°”ê¿€ ë§Œí•œ ì‚¬ê±´ì¸ê°€?
    6. ëˆ„êµ°ê°€(ì–´ëŠì§‘ë‹¨, ë‹¨ì²´)ì˜ ì˜ê²¬, ì˜ˆì¸¡ì´ ì•„ë‹Œ í˜„ì¬ì˜ ê°ê´€ì  ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ë‰´ìŠ¤ì¸ê°€?
    7. ê²¹ì¹˜ëŠ” ì£¼ì œëŠ” ì¤‘ë³µ ì„ ì •ì„ í”¼í•˜ê³  ë‹¤ì–‘í•œ ì´ìŠˆë¥¼ ì„ ì •í•˜ëŠ”ê°€?
    
    [JSON ì˜ˆì‹œ]
    [
      {{
        "category": "KR"(keyword, summary ë‚´ìš©ì— ë§ê²Œ ë¶„ë¥˜) globalì€ ë¯¸êµ­ì´ ì•„ë‹Œ ë‚˜ë¼ì˜ ë‰´ìŠ¤ì„,
        "keyword": "ì‚¼ì„±ì „ì ì–´ë‹ì‡¼í¬, ë°˜ë„ì²´ ë¶€ì§„ ì‹¬í™”", 
        "summary": "-ì‚¼ì„±ì „ìê°€ 3ë¶„ê¸° ì˜ì—…ì´ìµì´ ì „ë…„ ëŒ€ë¹„ ëŒ€í­ ê°ì†Œí–ˆë‹¤ê³  ë°œí‘œ.\n-ë°˜ë„ì²´ ìˆ˜ìš” ë‘”í™”ê°€ ì£¼ìš” ì›ì¸.\n-ê¸€ë¡œë²Œ ê²½ê¸° ì¹¨ì²´ ìš°ë ¤ì™€ ë§ë¬¼ë ¤ IT ì—…ê³„ ì „ë°˜ì— ë¶€ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ì—¬ì§€ê°€ ì¡´ì¬ ", 
        "links": [
          {{
            "url": "https://news.naver.com/...",
            "title": "ì‚¼ì„±ì „ì, 3ë¶„ê¸° ì˜ì—…ì´ìµ 2.4ì¡°ì›... ì‹œì¥ ì˜ˆìƒì¹˜ í•˜íšŒ"
          }}
        ]
      }}
    ]

    [ë‰´ìŠ¤ ë°ì´í„°]
    {json.dumps(raw_news_list, ensure_ascii=False)}
    """
    
    try:
        response = model.generate_content(
            prompt,
            generation_config={"response_mime_type": "application/json"}
        )
        response_text = response.text
        
        return json.loads(response_text, strict=False)
    except Exception as e:
        print(f"Error in Gemini processing: {e}")
        return []

def save_to_supabase(data):
    print(f"Saving {len(data)} items to Supabase...")
    if not data:
        print("No data to save.")
        return
    try:
        result = supabase.table("daily_news").insert(data).execute()
        print("Successfully saved!")
        return result
    except Exception as e:
        print(f"Error saving to Supabase: {e}")

def main():
    kr_news = fetch_naver_finance_main()
    us_news = fetch_yahoo_finance_stable()
    
    all_news = kr_news + us_news
    print(f"Total collected raw news: {len(all_news)} items")
    
    final_news = process_news_with_gemini(all_news)
    
    if final_news:
        print("Top 5 News Selected:")
        for idx, item in enumerate(final_news):
            print(f"{idx+1}. [{item['category']}] {item['keyword']}")
        save_to_supabase(final_news)
        
        # í‘¸ì‹œ ì•Œë¦¼ ì „ì†¡
        try:
            send_push_to_all(
                title="ğŸ“° ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ ì—…ë°ì´íŠ¸",
                body=f"AIê°€ ì„ ì •í•œ ì˜¤ëŠ˜ì˜ í•µì‹¬ ë‰´ìŠ¤ 5ê°œê°€ ë„ì°©í–ˆìŠµë‹ˆë‹¤: {final_news[0]['keyword']} ì™¸ 4ê±´",
                url="/news/daily-report"
            )
        except Exception as e:
            print(f"Failed to send push: {e}")
    else:
        print("Failed to generate news summary.")

if __name__ == "__main__":
    main()