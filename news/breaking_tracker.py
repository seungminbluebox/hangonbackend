import os
import sys
import time
import json
import calendar
import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
from supabase import create_client, Client
from newspaper import Article, Config
from google import genai

# ìƒìœ„ ë””ë ‰í† ë¦¬ ì°¸ì¡° ì¶”ê°€ (ë¡œì»¬ config.py ìš°ì„ ê¶Œì„ ìœ„í•´ sys.path ë§¨ ì•ì— ì¶”ê°€)
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from config import GEMINI_MODEL_NAME
from news.push_notification import send_push_notification

load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
GOOGLE_API_KEY = os.getenv("GEMINI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

client = genai.Client(api_key=GOOGLE_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ê°ì‹œí•  ë‰´ìŠ¤ ì†ŒìŠ¤ (RSS) - ì‹¤ì‹œê°„ 'ì†ë³´' ì „ìš© ì‹œìŠ¤í…œìœ¼ë¡œ ì „ë©´ êµì²´
RSS_FEEDS = [
    # 1. Google News - ì´ˆë‹¨ìœ„ ì†ë³´ ê²€ìƒ‰ (ê²€ìƒ‰ ì¿¼ë¦¬ì— 'breaking news' ê°•ì œ)
    "https://news.google.com/rss/search?q=intitle:%22breaking+news%22+OR+intitle:%22ì†ë³´%22+when:1h&hl=en-US&gl=US&ceid=US:en",
    
    # 2. Yahoo Finance - ì†ë³´(Latest) ì „ìš© ì„¹ì…˜ RSS
    "https://finance.yahoo.com/news/rss",
]

# ë©”ëª¨ë¦¬ ìƒì—ì„œ ì´ë¯¸ ì²˜ë¦¬í•œ ë‰´ìŠ¤ íƒ€ì„ìŠ¤íƒ¬í”„ ë˜ëŠ” ì œëª© ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
processed_news = set()

def get_recent_news_titles():
    """DBì—ì„œ ìµœê·¼ 20ê°œì˜ ì†ë³´ ì œëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        res = supabase.table("breaking_news").select("title").order("created_at", desc=True).limit(20).execute()
        return [item['title'] for item in res.data]
    except Exception as e:
        print(f"Error fetching recent titles: {e}")
        return []

def fetch_latest_headlines():
    headlines = []
    # 1. ê¸°ì¤€ ì‹œê°„ ì„¤ì • (ëª¨ë‘ UTCë¡œ í†µì¼í•˜ì—¬ ì •í™•í•˜ê²Œ 30ë¶„ í•„í„°ë§)
    now_utc = datetime.now(timezone.utc)
    time_limit_utc = now_utc - timedelta(minutes=30)
    
    # ì†ë³´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•µì‹¬ í‚¤ì›Œë“œ (ì…êµ¬ ì»·ìš©)
    BREAKING_KEYWORDS = ["ì†ë³´", "breaking", "urgent", "just in", "alert", "flash", "ê¸‰ë³´", "ê³µì‹œ", "[íŠ¹ì§•ì£¼]"]
    
    custom_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    
    # 2. RSS í”¼ë“œ ìˆ˜ì§‘ (Global/Google ì†ë³´ í”¼ë“œ)
    for i, url in enumerate(RSS_FEEDS, 1):
        try:
            feed = feedparser.parse(url, agent=custom_agent)
            entries_found = len(feed.entries)
            print(f"ğŸ“¡ Source {i} (RSS) checking: {entries_found} entries found.")
            
            for entry in feed.entries:
                title_lower = entry.title.lower()
                
                # [í•„í„° 1] ì œëª©ì— 'ì†ë³´' ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²ƒë§Œ 1ì°¨ ì„ ë³„
                if not any(kw in title_lower for kw in BREAKING_KEYWORDS):
                    continue

                pub_datetime_utc = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_ts = calendar.timegm(entry.published_parsed)
                    pub_datetime_utc = datetime.fromtimestamp(pub_ts, tz=timezone.utc)
                
                is_recent = False
                if pub_datetime_utc:
                    if pub_datetime_utc >= time_limit_utc:
                        is_recent = True
                else:
                    is_recent = True
                
                if is_recent:
                    headlines.append({
                        "title": entry.title,
                        "link": entry.link,
                        "source": "Global/RSS Feed"
                    })
        except Exception as e:
            print(f"Error fetching RSS {url}: {e}")

    # 3. êµ­ë‚´ ì†ë³´ (ë„¤ì´ë²„ ê¸ˆìœµ) - KSTë¥¼ UTCë¡œ ë³€í™˜í•˜ì—¬ ë™ê¸°í™”
    try:
        url = "https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258"
        headers = {"User-Agent": custom_agent}
        res = requests.get(url, headers=headers)
        res.encoding = 'cp949' 
        soup = BeautifulSoup(res.text, "html.parser")
        
        kst = timezone(timedelta(hours=9))
        news_items = soup.select("ul.realtimeNewsList > li")
        print(f"ğŸ‡°ğŸ‡· Naver Finance checking: {len(news_items)} entries found.")
        
        for item in news_items:
            subject_tag = item.select_one(".articleSubject a")
            wdate_tag = item.select_one(".wdate")
            
            if subject_tag and wdate_tag:
                title = subject_tag.text.strip()
                title_lower = title.lower()

                # [í•„í„° 1] ë„¤ì´ë²„ ë‰´ìŠ¤ë„ ì œëª©ì— 'ì†ë³´' í‚¤ì›Œë“œê°€ ìˆëŠ” ê²ƒë§Œ ì„ ë³„
                if not any(kw in title_lower for kw in BREAKING_KEYWORDS):
                    continue

                link = "https://finance.naver.com" + subject_tag['href']
                date_str = wdate_tag.text.strip().replace(".", "-")
                
                try:
                    pub_time_kst = datetime.strptime(date_str, "%Y-%m-%d %H:%M").replace(tzinfo=kst)
                    pub_time_utc = pub_time_kst.astimezone(timezone.utc)
                    
                    if pub_time_utc >= time_limit_utc:
                        headlines.append({
                            "title": title,
                            "link": link,
                            "source": "Naver Finance (Strict)"
                        })
                except: pass
    except Exception as e:
        print(f"Error fetching Naver breaking news: {e}")

    return headlines

def filter_breaking_news(headlines, recent_titles):
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¤‘ ì§„ì§œ 'ì†ë³´' ê°€ì¹˜ê°€ ìˆëŠ” ê²ƒë§Œ ì„ ë³„í•©ë‹ˆë‹¤.
    ìµœê·¼ì— ì´ë¯¸ ë³´ë„ëœ ë‚´ìš©ê³¼ ê²¹ì¹˜ëŠ”ì§€ ì²´í¬í•©ë‹ˆë‹¤.
    """
    if not headlines:
        return []

    prompt = f"""
    ë‹¹ì‹ ì€ ë¸”ë£¸ë²„ê·¸ì™€ ë¡œì´í„°ì˜ ìˆ˜ì„ ì—ë””í„°ë¥¼ í•©ì³ë†“ì€ ë“¯í•œ ì´ˆì—˜ë¦¬íŠ¸ ê²½ì œ ì†ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
    í˜„ì¬ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ëª©ë¡ì—ì„œ 'ì§„ì§œ ì‹œì¥ì„ ë’¤í”ë“¤ íŒŒê´´ë ¥ ìˆëŠ” ì†ë³´'ë§Œ ë‹¨ í•œë‘ ê°œ, í˜¹ì€ í•˜ë‚˜ë„ ì„ íƒí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    ê°€ë³ê³  í”í•œ ì†Œì‹ì€ ê³¼ê°íˆ ë²„ë¦¬ì„¸ìš”.

    [í›„ë³´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸]
    {json.dumps(headlines, ensure_ascii=False)}

    [ìµœê·¼ ë³´ë„ëœ ì†ë³´ (ì¤‘ë³µ ê¸ˆì§€)]
    {json.dumps(recent_titles, ensure_ascii=False)}

    [ì—„ê²©í•˜ë˜ ìœ ì—°í•œ í•„í„°ë§ ê¸°ì¤€]
    1. **í•„í„°ë§ ëŒ€ìƒ (Skip)**: ë‹¨ìˆœ ì‹œí™© ìš”ì•½, ì¼ë°˜ì ì¸ ì¦ì‹œ ì „ë§, ì†Œí˜•ì£¼ ë‰´ìŠ¤, ì¼ìƒì ì¸ í™ë³´ì„± ê¸°ì‚¬, ì´ë¯¸ ì•Œë ¤ì§„ ì •ë³´ì˜ ë‹¨ìˆœ ì¬íƒ•.
    2. **ìš°ì„  ìˆœìœ„ (Must Include)**:
       - **í•µì‹¬ ì§€í‘œ**: CPI, PCE, ê³ ìš©ë³´ê³ ì„œ, ê¸ˆë¦¬ ê²°ì • ë“± ì£¼ìš” ê²½ì œì§€í‘œ ê³µì‹ ë°œí‘œ ì¦‰ì‹œ.
       - **ì‹œì¥ ë³€ë™**: í™˜ìœ¨ ê¸‰ë“±ë½, êµ­ì±„ ê¸ˆë¦¬ í­ë“±, ì£¼ìš” ì§€ìˆ˜(KOSPI, NASDAQ)ì˜ ìœ ì˜ë¯¸í•œ ë³€ë™ ë° ì¶”ì„¸ ì „í™˜.
       - **ê¸°ì—… ì†ë³´**: ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤, ì• í”Œ, ì—”ë¹„ë””ì•„ ë“± ëŒ€ì¥ì£¼ë“¤ì˜ 'ê¸°ëŒ€ì¹˜ë¥¼ í¬ê²Œ ë²—ì–´ë‚œ' ì‹¤ì  ë°œí‘œë‚˜ í•µì‹¬ ê³µì‹œ.
       - **ì •ì±…/ê¸´ê¸‰**: ì •ë¶€ì˜ ì¤‘ëŒ€ ì‹œì¥ ì •ì±… ë°œí‘œ, ê¸ˆìœµê¶Œ ê¸´ê¸‰ ìˆ˜í˜ˆ, ë˜ëŠ” ì‹¤ì œ ë°œìƒí•œ ì§€ì •í•™ì  ì¶©ê²©.
    3. **ë¬´ê²Œê° íŒë‹¨**: 'ì´ ì†Œì‹ì„ ì•Œê²Œ ë¨ìœ¼ë¡œì¨ íˆ¬ììê°€ ì¦‰ê°ì ìœ¼ë¡œ í–‰ë™ì„ ê³ ë¯¼í•˜ê²Œ ë§Œë“œëŠ”ê°€?'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìœ¼ì„¸ìš”. 
    4. **ì¤‘ë³µ ë°°ì œ**: ì´ë¯¸ ë³´ë„ëœ ëª©ë¡ê³¼ í•µì‹¬ í‚¤ì›Œë“œê°€ ê²¹ì¹˜ë”ë¼ë„, 'ìƒˆë¡œìš´ ìˆ˜ì¹˜ê°€ ë°œí‘œ'ë˜ì—ˆê±°ë‚˜ 'ìƒí™©ì´ ê¸‰ì§„ì „'ëœ ê²ƒì´ë¼ë©´ í¬í•¨í•˜ì„¸ìš”.

    [ì¶œë ¥ í˜•ì‹]
    - ë°˜ë“œì‹œ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. 
    - ê¸°ì¤€ì— ë¶€í•©í•˜ëŠ” ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ []ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
    - ì¤‘ìš”ë„(importance_score): ê¸°ì‚¬ì˜ íŒŒê¸‰ë ¥ì— ë”°ë¼ 7~10ì ìœ¼ë¡œ ë¶€ì—¬í•˜ì„¸ìš”. (7ì  ë¯¸ë§Œì€ ëˆ„ë½)
    - title: í•œêµ­ì–´ë¡œ 15ì ì´ë‚´, ì œëª©ë§Œ ë³´ê³ ë„ ìƒí™©ì´ íŒŒì•…ë˜ê²Œ ëª…í™•í•˜ê³  ê°•ë ¬í•˜ê²Œ. ë¬¸ì¥ ëì— ë¬¸ì¥ì— ì–´ìš¸ë¦¬ëŠ” ì´ëª¨ì§€ í•˜ë‚˜ ì¶”ê°€.
    - content: ìˆ˜ì¹˜ë‚˜ í•µì‹¬ íŒ©íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ 1~2ë¬¸ì¥ìœ¼ë¡œ ì••ì¶•.
    - category: 'market', 'indicator', 'geopolitics', 'corporate' ì¤‘ ìµœì ì˜ ì¹´í…Œê³ ë¦¬ ì„ íƒ.
    """

    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL_NAME,
            contents=prompt
        )
        text = response.text
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0]
        elif "```" in text:
            text = text.split("```")[1].split("```")[0]
        
        candidates = json.loads(text.strip())
        return candidates
    except Exception as e:
        print(f"AI filtering error: {e}")
        return []

def save_and_notify(news_item):
    """
    DBì— ì €ì¥í•˜ê³  ì‹¤ì‹œê°„ ì•Œë¦¼ì„ ë³´ëƒ…ë‹ˆë‹¤.
    """
    try:
        # ì•ˆì „í•œ í‚¤ ì°¸ì¡° (KeyError ë°©ì§€)
        title = news_item.get('title')
        content = news_item.get('content', '')
        score = news_item.get('importance_score', 7)
        category = news_item.get('category', 'market')
        url = news_item.get('original_url', '')

        if not title:
            return

        # ì¤‘ë³µ ì²´í¬
        res = supabase.table("breaking_news").select("id").eq("title", title).execute()
        if res.data:
            print(f"Skipping duplicate: {title}")
            return

        # 1. DB ì €ì¥ (ì´ë¯¸ì§€ ì¶”ì¶œ ì¶”ê°€)
        image_url = None
        if url:
            try:
                config = Config()
                config.browser_user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                config.request_timeout = 5
                article = Article(url, config=config)
                article.download()
                article.parse()
                image_url = article.top_image
            except Exception as e:
                print(f"Image fetch error: {e}")

        data = {
            "title": title,
            "content": content,
            "importance_score": score,
            "category": category,
            "original_url": url,
            "image_url": image_url
        }
        supabase.table("breaking_news").insert(data).execute()
        print(f"ğŸš€ New Breaking News Saved: {title}")

        # 2. ì‹¤ì‹œê°„ í‘¸ì‹œ ì•Œë¦¼ (ì¹´í…Œê³ ë¦¬: breaking_news)
        send_push_notification(
            title=f"[ì†ë³´] {title}",
            body=content,
            url="/live", # ì†ë³´ íƒ€ì„ë¼ì¸ ì „ìš© í˜ì´ì§€ë¡œ ë§í¬
            category="breaking_news"
        )
    except Exception as e:
        print(f"Error in save_and_notify: {e}")

def main():
    print("ğŸ¬ 24/7 Breaking News Tracker is running...")
    
    while True:
        try:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            print(f"[{now}] Monitoring for updates...")
            
            # 1. í—¤ë“œë¼ì¸ ìˆ˜ì§‘
            raw_headlines = fetch_latest_headlines()
            
            # 2. ì¤‘ë³µ í•„í„°ë§ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
            new_headlines = []
            for h in raw_headlines:
                if h['title'] not in processed_news:
                    new_headlines.append(h)
                    processed_news.add(h['title'])
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬ (ìµœê·¼ 500ê°œë§Œ ìœ ì§€)
            if len(processed_news) > 500:
                processed_news.clear()

            # 3. DBì—ì„œ ìµœê·¼ ë³´ë„ëœ ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ë¬¸ë§¥ íŒŒì•… ë° ì¤‘ë³µ ë°©ì§€ìš©)
            recent_titles = get_recent_news_titles()

            # 4. AI í•„í„°ë§ ë° ìš”ì•½ (ìµœê·¼ ë³´ë„ ëª©ë¡ ì „ë‹¬)
            if new_headlines:
                print(f"ğŸ” Analyzing {len(new_headlines)} new headlines with AI...")
                breaking_items = filter_breaking_news(new_headlines, recent_titles)
                
                if not breaking_items:
                    print("ğŸƒ No high-impact breaking news found in this batch.")
                
                # 5. ì €ì¥ ë° ì•Œë¦¼
                for item in breaking_items:
                    save_and_notify(item)
            else:
                print("ğŸ’¤ No new headlines to analyze.")
            
            # 6. ì£¼ê¸° ì„¤ì • (120ì´ˆ - 2ë¶„ë§ˆë‹¤ ì²´í¬)
            # ìœ ë™ì ìœ¼ë¡œ ì¡°ì ˆ ê°€ëŠ¥
            time.sleep(5)
            
        except KeyboardInterrupt:
            print("Tracker stopped by user.")
            break
        except Exception as e:
            print(f"Main loop error: {e}")
            time.sleep(60)

if __name__ == "__main__":
    main()
