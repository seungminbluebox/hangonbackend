import feedparser
from google import genai
from supabase import create_client, Client
import json
import os
from dotenv import load_dotenv

# 1. í™˜ê²½ ë³€ìˆ˜ ë° í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
client = genai.Client(api_key=GEMINI_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 2. ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ (êµ¬ê¸€ ë‰´ìŠ¤ RSS í†µì¼)
def fetch_all_candidate_news():
    candidates = {"KR": [], "US": [], "Global": []}

    # A. í•œêµ­ (êµ¬ê¸€ ë‰´ìŠ¤ - ê²½ì œ í‚¤ì›Œë“œ)
    try:
        # ë” ì •í™•í•œ ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ìœ„í•´ í‚¤ì›Œë“œ ë³´ê°•
        kr_feed = feedparser.parse("https://news.google.com/rss/search?q=%EA%B2%BD%EC%A0%9C+%EA%B8%88%EB%A6%AC+%EC%8B%9C%EC%9E%A5+when:1d&hl=ko&gl=KR&ceid=KR:ko")
        for entry in kr_feed.entries[:15]:
            candidates["KR"].append({"title": entry.title, "url": entry.link})
    except Exception as e:
        print(f"í•œêµ­ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—ëŸ¬: {e}")

    # B. ë¯¸êµ­ (êµ¬ê¸€ ë‰´ìŠ¤ US Business)
    try:
        us_feed = feedparser.parse("https://news.google.com/rss/search?q=business+finance+stock+market+when:1d&hl=en-US&gl=US&ceid=US:en")
        for entry in us_feed.entries[:15]:
            candidates["US"].append({"title": entry.title, "url": entry.link})
    except Exception as e:
        print(f"ë¯¸êµ­ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—ëŸ¬: {e}")

    # C. ê¸€ë¡œë²Œ (êµ¬ê¸€ ë‰´ìŠ¤ World Economy)
    try:
        global_feed = feedparser.parse("https://news.google.com/rss/search?q=World+Economy+Outlook+when:1d&hl=ko&gl=KR&ceid=KR:ko")
        for entry in global_feed.entries[:15]:
            candidates["Global"].append({"title": entry.title, "url": entry.link})
    except Exception as e:
        print(f"ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—ëŸ¬: {e}")

    return candidates


# 3. Gemini í•„í„°ë§ ë° ìš”ì•½ í•¨ìˆ˜
def get_curated_summary(news_list):
    # 1. ê³ ìœ  IDì™€ í•¨ê»˜ ë§¤í•‘ ë°ì´í„° ìƒì„±
    id_map = {}
    all_candidates_text = ""
    for cat in ["KR", "US", "Global"]:
        all_candidates_text += f"\n[{cat} ë‰´ìŠ¤ í›„ë³´]\n"
        for i, item in enumerate(news_list[cat]):
            news_id = f"{cat}_{i}"
            id_map[news_id] = item
            all_candidates_text += f"ID: {news_id}\nì œëª©: {item['title']}\n\n"

    prompt = f"""
    ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ê²½ì œ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ê¸°ì‚¬ í›„ë³´ë“¤ ì¤‘ì—ì„œ 
    ë°˜ë“œì‹œ [í•œêµ­ 2ê°œ, ë¯¸êµ­ 2ê°œ, ê¸€ë¡œë²Œ 1ê°œ]ì˜ ë¹„ìœ¨ì„ ì§€ì¼œ ì´ 5ê°œì˜ í•µì‹¬ ë‰´ìŠ¤ë¥¼ ì„ ì •í•˜ê³  ìš”ì•½í•˜ì„¸ìš”.

    [ë‰´ìŠ¤ í›„ë³´ ëª©ë¡]
    {all_candidates_text}
    
    [ì„ ì • ê¸°ì¤€]
    1. ë‰´ìŠ¤ê°€ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë ¥ì´ í°ê°€?
    2. íˆ¬ììë“¤ì´ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  í•µì‹¬ ì •ë³´ì¸ê°€(ex ê¸ˆë¦¬ ë³€ë™, ì •ì±… ë°œí‘œ, í™˜ìœ¨ ë³€ë™ ë“±)?
    3. ë‹¨ê¸°ì  ì´ìŠˆê°€ ì•„ë‹Œ ì¤‘ì¥ê¸°ì  ê´€ì ì—ì„œ ì¤‘ìš”í•œê°€?
    4. êµ­ê°€ ì •ì±…, ê¸ˆë¦¬, í™˜ìœ¨ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ì£¼ëŠ”ê°€?
    5. ê¸€ë¡œë²Œ ë¹…í…Œí¬ë‚˜ ì£¼ìš” ì‚°ì—…ì˜ íŒë„ë¥¼ ë°”ê¿€ ë§Œí•œ ì‚¬ê±´ì¸ê°€?
    6. ì¤‘ë³µë˜ëŠ” ë‚´ìš© ì—†ì´ ë‹¤ì–‘í•œ ì´ìŠˆë¥¼ ë‹¤ë£¨ê³  ìˆëŠ”ê°€?
    7. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ì—ì„œ ë‚˜ì˜¨ ë‰´ìŠ¤ì¸ê°€?
    8. ê²°ê³¼ë¬¼ì€ ë°˜ë“œì‹œ í•œêµ­ 2ê°œ, ë¯¸êµ­ 2ê°œ, ê¸€ë¡œë²Œ 1ê°œì—¬ì•¼ í•©ë‹ˆë‹¤.
    9. ê¸°ìì˜ ì˜ê²¬ì´ ì•„ë‹Œ ê°ê´€ì  ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ë‰´ìŠ¤ì—¬ì•¼ í•©ë‹ˆë‹¤.
    10. ëª¨ë“  ìš”ì•½ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    11. ì´ 5ê°œë¥¼ ì„ ì •: í•œêµ­ 2ê°œ, ë¯¸êµ­ 2ê°œ, ê¸€ë¡œë²Œ 1ê°œ í•„ìˆ˜.
    12. ë‰´ìŠ¤ ì„ ì • ê¸°ì¤€: ì¤‘ì¥ê¸°ì  ì‹œì¥ ì˜í–¥ë ¥, íˆ¬ì ì¸ì‚¬ì´íŠ¸ê°€ í’ë¶€í•œ ë‰´ìŠ¤.
    13. ìš”ì•½ ìŠ¤íƒ€ì¼: '~í•¨', '~ìŒ'ìœ¼ë¡œ ëë‚˜ëŠ” ê°œì¡°ì‹ ìš”ì•½ (3ê°œ í¬ì¸íŠ¸).
    14. ë°˜ë“œì‹œ ì œê³µëœ ì›ë³¸ URLì„ ì‚¬ìš©í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    15. ê° ë‰´ìŠ¤ í›„ë³´ëŠ” IDë¡œ êµ¬ë¶„ë˜ì–´ ìˆìœ¼ë©°, ì œëª©ê³¼ URLì´ í•œ ìŒì…ë‹ˆë‹¤.
    16. ì„ ì •ëœ ë‰´ìŠ¤ì˜ ìš”ì•½ì„ ì‘ì„±í•  ë•Œ, í•´ë‹¹ IDì— ê·€ì†ëœ ì›ë³¸ URLì„ ì ˆëŒ€ë¡œ ë³€ê²½í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì œëª©ê³¼ ì„ì§€ ë§ˆì„¸ìš”.
    17. ê° ë‰´ìŠ¤ì˜ keywordì— ë§ˆì§€ë§‰ì— keywordì— ë§ëŠ” ì´ëª¨ì§€ ì‚¬ìš©
    18. ë¹„ìŠ·í•œ ì£¼ì œëŠ” í”¼í•˜ê³  ë‹¤ì–‘í•œ ì´ìŠˆ ì„ ì •
    ê²°ê³¼ì—ëŠ” ë°˜ë“œì‹œ ì„ ì •í•œ ë‰´ìŠ¤ì˜ 'ID'ë¥¼ 'selected_id' í•„ë“œì— ë‹´ì•„ ë°˜í™˜í•˜ì„¸ìš”.

    [ì¶œë ¥ í˜•ì‹]
    ë°˜ë“œì‹œ JSON ìŠ¤í‚¤ë§ˆ í˜•ì‹ì„ ì¤€ìˆ˜í•˜ì„¸ìš”.
    [
      {{
        "category": "KR | US | Global",
        "keyword": "ì´ìŠˆë¥¼ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥",
        "summary": "- ìš”ì•½ ë‚´ìš© 1\\n- ìš”ì•½ ë‚´ìš© 2\\n- ì‹œì¥ ì „ë§/ì‹œì‚¬ì ",
        "selected_id": "ì„ ì •í•œ ë‰´ìŠ¤ì˜ ID (ì˜ˆ: KR_0)"
      }}
    ]
    """

    response = client.models.generate_content(
        model="gemini-2.0-flash", # ì†ë„ì™€ ì„±ëŠ¥ì´ ê· í˜• ì¡íŒ ëª¨ë¸
        contents=prompt,
        config={'response_mime_type': 'application/json'}
    )
    
    raw_results = json.loads(response.text)
    
    # 2. IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›ë³¸ URL ë§¤ì¹­ (íŒŒì´ì¬ì—ì„œ ìˆ˜í–‰)
    final_results = []
    for res in raw_results:
        news_id = res.get("selected_id")
        original_news = id_map.get(news_id)
        
        if original_news:
            res["links"] = [{"title": original_news["title"], "url": original_news["url"]}]
            # selected_idëŠ” DB ì €ì¥ ì‹œ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì‚­ì œ ê°€ëŠ¥
            del res["selected_id"]
            final_results.append(res)
            
    return final_results

# 4. ì‹¤í–‰ ë¡œì§
def main():
    print("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    candidates = fetch_all_candidate_news()
    if not candidates:
        print("âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    print(f"ğŸ§ {len(candidates)}ê°œì˜ í›„ë³´ ì¤‘ 5ê°œ ì„ ë³„ ë° ìš”ì•½ ì¤‘...")
    try:
        final_news = get_curated_summary(candidates)
        # 5. Supabase ì €ì¥
        for item in final_news:
            supabase.table("daily_news").insert(item).execute()
        
        print("âœ… ì„±ê³µì ìœ¼ë¡œ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ìš”ì•½ ë° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

main()